version: '3.8'

services:
  # Training service
  trainer:
    build:
      context: .
      dockerfile: Dockerfile
    image: sharknet-trainer:latest
    container_name: sharknet_trainer
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./output:/app/output
      - ./saved_models:/app/saved_models
      - ./prototype:/app/prototype
    environment:
      - MODEL_NAME=unsloth/meta-llama-3.1-8b-instruct-bnb-4bit
      - MAX_SEQ_LENGTH=512
      - LEARNING_RATE=2e-5
      - BATCH_SIZE=4
      - NUM_EPOCHS=3
      - GRADIENT_ACCUMULATION_STEPS=4
      - TRAIN_TEST_SPLIT=0.8
      - TRAINING_MODE=grpo
      - CUDA_VISIBLE_DEVICES=0
      - DATA_PATH=prototype/RLdata_unix.txt
    restart: "no"

  # Inference service with Web UI
  inference:
    build:
      context: .
      dockerfile: Dockerfile.inference
    image: sharknet-inference:latest
    container_name: sharknet_inference
    ports:
      - "7860:7860"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./output:/app/output
      - ./saved_models:/app/saved_models
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MAX_MEMORY=16GB
    restart: unless-stopped

  # Optional service for experiment tracking
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.11.0
    container_name: sharknet_mlflow
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow:/mlflow
    environment:
      - MLFLOW_TRACKING_URI=http://localhost:5000
    command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri /mlflow
    restart: unless-stopped

networks:
  default:
    name: sharknet
    driver: bridge 